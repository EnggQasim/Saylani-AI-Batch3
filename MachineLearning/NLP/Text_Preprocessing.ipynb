{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mqasim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import unidecode\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from autocorrect import Speller\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    79582\n",
      "3    32927\n",
      "1    27273\n",
      "4     9206\n",
      "0     7072\n",
      "Name: Sentiment, dtype: int64\n",
      "Number of Data points :  14144\n",
      "Number of features : 2\n",
      "features : ['Phrase' 'Sentiment']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>would have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>have a hard time sitting through this one</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>self-glorification and a manipulative whitewash</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Trouble Every Day is a plodding mess .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Phrase  Sentiment\n",
       "101    would have a hard time sitting through this one          0\n",
       "103          have a hard time sitting through this one          0\n",
       "157  Aggressive self-glorification and a manipulati...          0\n",
       "159    self-glorification and a manipulative whitewash          0\n",
       "201             Trouble Every Day is a plodding mess .          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Dataset\n",
    "Df = pd.read_csv(\"train.tsv\", sep='\\t')\n",
    "print(Df.Sentiment.value_counts())\n",
    "df = pd.concat([Df[Df.Sentiment==0], Df[Df.Sentiment==4].sample(7072)])[[\"Phrase\",\"Sentiment\"]]\n",
    "Df=df\n",
    "Df.head()\n",
    "# Df = pd.read_csv('New Task.csv', encoding = 'latin-1')\n",
    "print('Number of Data points : ', Df.shape[0])\n",
    "print('Number of features :', Df.shape[1])\n",
    "print('features :', Df.columns.values)\n",
    "# Show Dataset\n",
    "Df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This command tells information about the attributes of Dataset.\n",
    "Df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14144.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Sentiment\n",
       "count  14144.000000\n",
       "mean       2.000000\n",
       "std        2.000071\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        2.000000\n",
       "75%        4.000000\n",
       "max        4.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows statistics for every numerical column in our dataset.\n",
    "Df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check type of Dataframe attribute that has to processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type of attribute \"Content\"\n",
    "type(Df['Phrase'])\n",
    "\n",
    "#Type of attribute \"Title\"\n",
    "# type(Df['Title'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove newlines & tabs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_newlines_tabs(text):\n",
    "    \"\"\"\n",
    "    This function will remove all the occurrences of newlines, tabs, and combinations like: \\\\n, \\\\.\n",
    "    \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of newlines, tabs, \\\\n, \\\\ characters.\n",
    "        \n",
    "    Example:\n",
    "    Input : This is her \\\\ first day at this place.\\n Please,\\t Be nice to her.\\\\n\n",
    "    Output : This is her first day at this place. Please, Be nice to her. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Replacing all the occurrences of \\n,\\\\n,\\t,\\\\ with a space.\n",
    "    Formatted_text = text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n",
    "    return Formatted_text\n",
    "# len of data :- 1618647 lac words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip Html Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags(text):\n",
    "    \"\"\" \n",
    "    This function will remove all the occurrences of html tags from the text.\n",
    "    \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of html tags.\n",
    "        \n",
    "    Example:\n",
    "    Input : This is a nice place to live. <IMG>\n",
    "    Output : This is a nice place to live.  \n",
    "    \"\"\"\n",
    "    # Initiating BeautifulSoup object soup.\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    # Get all the text other than html tags.\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "    return stripped_text\n",
    "# len of string:- 1616053 lac words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "    \"\"\"\n",
    "    This function will remove all the occurrences of links.\n",
    "    \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of all types of links.\n",
    "        \n",
    "    Example:\n",
    "    Input : To know more about cats and food & website: catster.com  visit: https://catster.com//how-to-feed-cats\n",
    "    Output : To know more about cats and food & website: visit:     \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Removing all the occurrences of links that starts with https\n",
    "    remove_https = re.sub(r'http\\S+', '', text)\n",
    "    # Remove all the occurrences of text that ends with .com\n",
    "    remove_com = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n",
    "    return remove_com\n",
    "# len of words:- 1616053"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove WhiteSpaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    \"\"\" This function will remove \n",
    "        extra whitespaces from the text\n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after extra whitespaces removed .\n",
    "        \n",
    "    Example:\n",
    "    Input : How   are   you   doing   ?\n",
    "    Output : How are you doing ?     \n",
    "        \n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'\\s+') \n",
    "    Without_whitespace = re.sub(pattern, ' ', text)\n",
    "    # There are some instances where there is no space after '?' & ')', \n",
    "    # So I am replacing these with one space so that It will not consider two words as one token.\n",
    "    text = Without_whitespace.replace('?', ' ? ').replace(')', ') ')\n",
    "    return text    \n",
    "# len of words:- 1596248 lac words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Remove Accented Characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for accented characters removal\n",
    "def accented_characters_removal(text):\n",
    "    # this is a docstring\n",
    "    \"\"\"\n",
    "    The function will remove accented characters from the \n",
    "    text contained within the Dataset.\n",
    "       \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" with removed accented characters.\n",
    "        \n",
    "    Example:\n",
    "    Input : Málaga, àéêöhello\n",
    "    Output : Malaga, aeeohello    \n",
    "        \n",
    "    \"\"\"\n",
    "    # Remove accented characters from text using unidecode.\n",
    "    # Unidecode() - It takes unicode data & tries to represent it to ASCII characters. \n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "# Len of data:- 1593952 lac of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Case Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for text lowercasing\n",
    "def lower_casing_text(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    The function will convert text into lower case.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "         value: text in lowercase\n",
    "         \n",
    "    Example:\n",
    "    Input : The World is Full of Surprises!\n",
    "    Output : the world is full of surprises!\n",
    "    \n",
    "    \"\"\"\n",
    "    # Convert text to lower case\n",
    "    # lower() - It converts all upperase letter of given string to lowercase.\n",
    "    text = text.lower()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Reduce repeated characters and punctuations¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for removing repeated characters and punctuations\n",
    "\n",
    "def reducing_incorrect_character_repeatation(text):\n",
    "    \"\"\"\n",
    "    This Function will reduce repeatition to two characters \n",
    "    for alphabets and to one character for punctuations.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Finally formatted text with alphabets repeating to \n",
    "        two characters & punctuations limited to one repeatition \n",
    "        \n",
    "    Example:\n",
    "    Input : Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n",
    "    Output : Reallyy, Greeaatt !?.;:)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Pattern matching for all case alphabets\n",
    "    Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
    "    \n",
    "    # Limiting all the  repeatation to two characters.\n",
    "    Formatted_text = Pattern_alpha.sub(r\"\\1\\1\", text) \n",
    "    \n",
    "    # Pattern matching for all the punctuations that can occur\n",
    "    Pattern_Punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
    "    \n",
    "    # Limiting punctuations in previously formatted string to only one.\n",
    "    Combined_Formatted = Pattern_Punct.sub(r'\\1', Formatted_text)\n",
    "    \n",
    "    # The below statement is replacing repeatation of spaces that occur more than two times with that of one occurrence.\n",
    "    Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)\n",
    "    return Final_Formatted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation for using some symbols in above regex expression\n",
    "**\\1** —> is equivalent to re.search(...). group(1). It Refers to first capturing group. \\1 matches the exact same text that was matched by the first capturing group.\n",
    "\n",
    "**{1,}** --> It means we are matching for repeatation that occurs more than one times. \n",
    "\n",
    "**DOTALL** -> It matches newline character as well unlike dot operator which matches everything in the given text except newline character. \n",
    "\n",
    "**sub()** --> This function is used to replace occurrences of a particular sub-string with another sub-string. This function takes as input the following: The sub-string to replace. The sub-string to replace with.\n",
    "\n",
    "**r'\\1\\1'** --> It limits all the repeatation to two characters.\n",
    "\n",
    "**r'\\1'** --> Limits all the repeatation to only one character.\n",
    "\n",
    "**{2,}** --> It means to match for repeatation that occurs more than two times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: Expand contraction words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "}\n",
    "# The code for expanding contraction words\n",
    "def expand_contractions(text, contraction_mapping =  CONTRACTION_MAP):\n",
    "    \"\"\"expand shortened words to the actual form.\n",
    "       e.g. don't to do not\n",
    "    \n",
    "       arguments:\n",
    "            input_text: \"text\" of type \"String\".\n",
    "         \n",
    "       return:\n",
    "            value: Text with expanded form of shorthened words.\n",
    "        \n",
    "       Example: \n",
    "       Input : ain't, aren't, can't, cause, can't've\n",
    "       Output :  is not, are not, cannot, because, cannot have \n",
    "    \n",
    "     \"\"\"\n",
    "    # Tokenizing text into tokens.\n",
    "    list_Of_tokens = text.split(' ')\n",
    "\n",
    "    # Checking for whether the given token matches with the Key & replacing word with key's value.\n",
    "    \n",
    "    # Check whether Word is in lidt_Of_tokens or not.\n",
    "    for Word in list_Of_tokens: \n",
    "        # Check whether found word is in dictionary \"Contraction Map\" or not as a key. \n",
    "         if Word in CONTRACTION_MAP: \n",
    "                # If Word is present in both dictionary & list_Of_tokens, replace that word with the key value.\n",
    "                list_Of_tokens = [item.replace(Word, CONTRACTION_MAP[Word]) for item in list_Of_tokens]\n",
    "                \n",
    "    # Converting list of tokens to String.\n",
    "    String_Of_tokens = ' '.join(str(e) for e in list_Of_tokens) \n",
    "    return String_Of_tokens     \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5: Remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for removing special characters\n",
    "def removing_special_characters(text):\n",
    "    \"\"\"Removing all the special characters except the one that is passed within \n",
    "       the regex to match, as they have imp meaning in the text provided.\n",
    "   \n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text with removed special characters that don't require.\n",
    "        \n",
    "    Example: \n",
    "    Input : Hello, K-a-j-a-l. Thi*s is $100.05 : the payment that you will recieve! (Is this okay?) \n",
    "    Output :  Hello, Kajal. This is $100.05 : the payment that you will recieve! Is this okay?\n",
    "    \n",
    "   \"\"\"\n",
    "    # The formatted text after removing not necessary punctuations.\n",
    "    Formatted_Text = re.sub(r\"[^a-zA-Z0-9:$-,%.?!]+\", ' ', text) \n",
    "    # In the above regex expression,I am providing necessary set of punctuations that are frequent in this particular dataset.\n",
    "    return Formatted_Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuations that I am considering Important as per my Dataset.\n",
    "**,.?!** --> These are some frequent punctuations that occurs a lot and needed to preserved as to understand the context of text.\n",
    "\n",
    "__:__ --> This one is also frequent as per the  Dataset. It is important to keep bcz it is giving sense whenever there is a occurrence of time like: **9:05 p.m.**\n",
    "\n",
    "**%** --> This one is also frequently used in many articles and telling more precisely about the data, facts & figures.\n",
    "\n",
    "**$** --> This one is used in many articles where prices are considered. So, omitting this symbol will not give much sense about those prices that left as just some numbers only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6: Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for removing stopwords\n",
    "stoplist = stopwords.words('english') \n",
    "stoplist = set(stoplist)\n",
    "def removing_stopwords(text):\n",
    "    \"\"\"This function will remove stopwords which doesn't add much meaning to a sentence \n",
    "       & they can be remove safely without comprimising meaning of the sentence.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text after omitted all stopwords.\n",
    "        \n",
    "    Example: \n",
    "    Input : This is Kajal from delhi who came here to study.\n",
    "    Output : [\"'This\", 'Kajal', 'delhi', 'came', 'study', '.', \"'\"] \n",
    "    \n",
    "   \"\"\"\n",
    "    # repr() function actually gives the precise information about the string\n",
    "    text = repr(text)\n",
    "    # Text without stopwords\n",
    "    No_StopWords = [word for word in word_tokenize(text) if word.lower() not in stoplist ]\n",
    "    # Convert list of tokens_without_stopwords to String type.\n",
    "    words_string = ' '.join(No_StopWords)    \n",
    "    return words_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking spellings for all the stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8: Correct mis-spelled words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for spelling corrections\n",
    "def spelling_correction(text):\n",
    "    ''' \n",
    "    This function will correct spellings.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text after corrected spellings.\n",
    "        \n",
    "    Example: \n",
    "    Input : This is Oberois from Dlhi who came heree to studdy.\n",
    "    Output : This is Oberoi from Delhi who came here to study.\n",
    "      \n",
    "    \n",
    "    '''\n",
    "    # Check for spellings in English language\n",
    "    spell = Speller(lang='en')\n",
    "    Corrected_text = spell(text)\n",
    "    return Corrected_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for lemmatization\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatization(text):\n",
    "    \"\"\"This function converts word to their root words \n",
    "       without explicitely cut down as done in stemming.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text having root words only, no tense form, no plural forms\n",
    "        \n",
    "    Example: \n",
    "    Input : text reduced \n",
    "    Output :  text reduce\n",
    "    \n",
    "   \"\"\"\n",
    "    # Converting words to their root forms\n",
    "    lemma = [lemmatizer.lemmatize(w,'v') for w in w_tokenizer.tokenize(text)]\n",
    "    return lemma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step9: Putting all in single function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing main function to merge all the preprocessing steps.\n",
    "def text_preprocessing(text, accented_chars=True, contractions=True, lemmatization_allow = True,\n",
    "                        extra_whitespace=True, newlines_tabs=True, repeatition=True, \n",
    "                       lowercase=True, punctuations=True, mis_spell=True,\n",
    "                       remove_html=True, links=True,  special_chars=True,\n",
    "                       stop_words=True):\n",
    "    \"\"\"\n",
    "    This function will preprocess input text and return\n",
    "    the clean text.\n",
    "    \"\"\"\n",
    "    \n",
    "    if newlines_tabs == True: #remove newlines & tabs.\n",
    "        Data = remove_newlines_tabs(text)\n",
    "        \n",
    "    if remove_html == True: #remove html tags\n",
    "        Data = strip_html_tags(Data)\n",
    "        \n",
    "    if links == True: #remove links\n",
    "        Data = remove_links(Data)\n",
    "        \n",
    "    if extra_whitespace == True: #remove extra whitespaces\n",
    "        Data = remove_whitespace(Data)\n",
    "        \n",
    "    if accented_chars == True: #remove accented characters\n",
    "        Data = accented_characters_removal(Data)\n",
    "        \n",
    "    if lowercase == True: #convert all characters to lowercase\n",
    "        Data = lower_casing_text(Data)\n",
    "        \n",
    "    if repeatition == True: #Reduce repeatitions   \n",
    "        Data = reducing_incorrect_character_repeatation(Data)\n",
    "        \n",
    "    if contractions == True: #expand contractions\n",
    "        Data = expand_contractions(Data)\n",
    "    \n",
    "    if punctuations == True: #remove punctuations\n",
    "        Data = removing_special_characters(Data)\n",
    "    \n",
    "    stoplist = stopwords.words('english') \n",
    "    stoplist = set(stoplist)\n",
    "    \n",
    "    if stop_words == True: #Remove stopwords\n",
    "        Data = removing_stopwords(Data)\n",
    "        \n",
    "    spell = Speller(lang='en')\n",
    "    \n",
    "    if mis_spell == True: #Check for mis-spelled words & correct them.\n",
    "        Data = spelling_correction(Data)\n",
    "        \n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# #     print(Data)\n",
    "#     if lemmatization_allow == True: #Converts words to lemma form.\n",
    "#         Data = lemmatization(Data)\n",
    "    \n",
    "           \n",
    "    return Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'would hard time sitting one '\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessing(\"would have a hard time sitting through this one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7072\n",
       "4    7072\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df['Phrase'] = Df['Phrase'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "would have a hard time sitting through this one\n",
      "have a hard time sitting through this one\n",
      "Aggressive self-glorification and a manipulative whitewash\n",
      "self-glorification and a manipulative whitewash\n",
      "Trouble Every Day is a plodding mess .\n",
      "is a plodding mess\n",
      "plodding mess\n",
      "could hate it for the same reason\n",
      "hate it\n",
      "hate\n",
      "is Oedekerk 's realization of his childhood dream to be in a martial-arts flick , and proves that sometimes the dreams of youth should remain just that\n",
      "baseball movies that try too hard to be mythic\n",
      "Hampered -- no , paralyzed -- by a self-indulgent script ... that aims for poetry and ends up sounding like satire .\n",
      "a self-indulgent script\n",
      "There 's very little sense to what 's going on here ,\n",
      "avoid\n",
      "almost feels as if the movie is more interested in entertaining itself than in amusing us .\n",
      "The movie 's progression into rambling incoherence gives new meaning to the phrase ` fatal script error . '\n",
      "The movie 's progression into rambling incoherence\n",
      "gives new meaning to the phrase ` fatal script error . '\n",
      "` fatal script error\n",
      "fatal script error\n",
      "Tartakovsky 's team has some freakish powers of visual charm , but the five writers slip into the modern rut of narrative banality .\n",
      "of narrative banality\n",
      "bad\n",
      "'m sure the filmmaker would disagree , but , honestly , I do n't see the point\n",
      "honestly , I do n't see the point\n",
      "clunky TV-movie approach\n",
      "Thanks largely to Williams , all the interesting developments are processed in 60 minutes -- the rest is just an overexposed waste of film .\n",
      "the rest is just an overexposed waste of film\n",
      "is just an overexposed waste of film\n",
      "an overexposed waste of film\n",
      "an overexposed waste\n",
      "Comes across as a relic from a bygone era , and its convolutions ... feel silly rather than plausible .\n",
      "as a relic from a bygone era , and its convolutions ... feel silly rather than plausible\n",
      "Run for your lives !\n",
      "Run for your lives\n",
      "makes your teeth hurt\n",
      "Its generic villains lack any intrigue -LRB- other than their funny accents -RRB- and the action scenes are poorly delivered\n",
      "Its generic villains lack any intrigue -LRB- other than their funny accents -RRB- and\n",
      "Downright transparent is the script 's endless assault of embarrassingly ham-fisted sex jokes that reek of a script rewrite designed to garner the film a `` cooler '' PG-13 rating .\n",
      "is the script 's endless assault of embarrassingly ham-fisted sex jokes that reek of a script rewrite designed to garner the film a `` cooler '' PG-13 rating .\n",
      "is the script 's endless assault of embarrassingly ham-fisted sex jokes that reek of a script rewrite designed to garner the film a `` cooler '' PG-13 rating\n",
      "is the script 's endless assault of embarrassingly ham-fisted sex jokes\n",
      "endless assault\n",
      "embarrassingly ham-fisted sex jokes\n",
      "impossible\n",
      "goes overboard with a loony melodramatic denouement in which a high school swimming pool substitutes for a bathtub .\n",
      "directed , barely ,\n",
      "'s lazy for a movie to avoid solving one problem by trying to distract us with the solution to another\n",
      "lazy for a movie\n",
      "of its music or comic antics , but through the perverse pleasure of watching Disney scrape the bottom of its own cracker barrel\n",
      "Disney scrape the bottom of its own cracker barrel\n",
      "Scarcely worth a mention apart from reporting on the number of tumbleweeds blowing through the empty theatres graced with its company .\n",
      "Scarcely worth a mention apart from reporting on the number of tumbleweeds blowing through the empty theatres\n",
      "Scarcely worth a mention\n",
      "Scarcely worth\n",
      "apart from reporting on the number of tumbleweeds blowing through the empty theatres\n",
      "on the number of tumbleweeds blowing through the empty theatres\n",
      "of tumbleweeds blowing through the empty theatres\n",
      "tumbleweeds blowing through the empty theatres\n",
      "`` Analyze That '' is one of those crass , contrived sequels that not only fails on its own , but makes you second-guess your affection for the original .\n",
      "Analyze That '' is one of those crass , contrived sequels that not only fails on its own , but makes you second-guess your affection for the original .\n",
      "'' is one of those crass , contrived sequels that not only fails on its own , but makes you second-guess your affection for the original .\n",
      "is one of those crass , contrived sequels that not only fails on its own , but makes you second-guess your affection for the original .\n",
      "of those crass , contrived sequels that not only fails on its own , but makes you second-guess your affection for the original\n",
      "those crass , contrived sequels that not only fails on its own , but makes you second-guess your affection for the original\n",
      "those crass , contrived sequels\n",
      "crass , contrived sequels\n",
      "that not only fails on its own , but makes you second-guess your affection for the original\n",
      "fails on its own , but makes you second-guess your affection for the original\n",
      "fails\n",
      "'s not a particularly good film\n",
      "is -RRB- the comedy equivalent of Saddam Hussein , and I 'm just about ready to go to the U.N. and ask permission for a preemptive strike .\n",
      "is -RRB- the comedy equivalent of Saddam Hussein , and I 'm just about ready to go to the U.N. and ask permission for a preemptive strike\n",
      "is so consistently unimaginative that probably the only way to have saved the film is with the aid of those wisecracking Mystery Science Theater 3000 guys .\n",
      "is so consistently unimaginative that probably the only way to have saved the film is with the aid of those wisecracking Mystery Science Theater 3000 guys\n",
      "A mawkish self-parody that plays like some weird Masterpiece Theater sketch with neither a point of view nor a compelling reason for being\n",
      "Takes one character we do n't like and another we do n't believe , and puts them into a battle of wills that is impossible to care about and is n't very funny\n",
      "do n't like\n",
      "another we do n't believe , and puts them into a battle of wills that is impossible to care about and is n't very funny\n",
      "is impossible to care about and is n't very funny\n",
      "then knock yourself out and enjoy the big screen postcard that is a self-glorified Martin Lawrence lovefest .\n",
      "The whole film has this sneaky feel to it -- as if the director is trying to dupe the viewer into taking it all as Very Important simply because the movie is ugly to look at and not a Hollywood product .\n",
      "this sneaky feel to it -- as if the director is trying to dupe the viewer into taking it all as Very Important simply because the movie is ugly to look at and not a Hollywood product\n",
      "to it -- as if the director is trying to dupe the viewer into taking it all as Very Important simply because the movie is ugly to look at and not a Hollywood product\n",
      "-- as if the director is trying to dupe the viewer into taking it all as Very Important simply because the movie is ugly to look at and not a Hollywood product\n",
      "as if the director is trying to dupe the viewer into taking it all as Very Important simply because the movie is ugly to look at and not a Hollywood product\n",
      "the director is trying to dupe the viewer into taking it all as Very Important simply because the movie is ugly to look at and not a Hollywood product\n",
      "is trying to dupe the viewer into taking it all as Very Important simply because the movie is ugly to look at and not a Hollywood product\n",
      "trying to dupe the viewer into taking it all as Very Important simply because the movie is ugly to look at and not a Hollywood product\n",
      "to dupe the viewer into taking it all as Very Important simply because the movie is ugly to look at and not a Hollywood product\n",
      "dupe the viewer into taking it all as Very Important simply because the movie is ugly to look at and not a Hollywood product\n",
      "simply because the movie is ugly to look at and not a Hollywood product\n",
      "the movie is ugly to look at and not a Hollywood product\n",
      "ugly to look at and not a Hollywood product\n",
      "bogged down in earnest dramaturgy\n",
      "cheap\n",
      "Violent , vulgar and forgettably\n",
      "vulgar\n",
      "at the expense of those who paid for it and those who pay to see it\n",
      "It 's hard to say who might enjoy this\n",
      "hard to say who might enjoy this\n",
      "The movie is silly beyond comprehension , and even if it were n't silly , it would still be beyond comprehension .\n",
      "The movie is silly beyond comprehension , and\n",
      "Poor editing , bad bluescreen , and ultra-cheesy dialogue highlight the radical action .\n",
      "Poor editing , bad bluescreen , and ultra-cheesy dialogue\n",
      "Poor editing , bad bluescreen , and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poor editing , bad bluescreen\n",
      "bad bluescreen\n",
      "completely misses its emotions\n",
      "failing , ultimately ,\n",
      "failing , ultimately\n",
      "There 's a scientific law to be discerned here that producers would be well to heed : Mediocre movies start to drag as soon as the action speeds up ; when the explosions start , they fall to pieces .\n",
      "There 's a scientific law to be discerned here that producers would be well to heed : Mediocre movies start to drag as soon as the action speeds up ; when the explosions start , they fall to pieces\n",
      "There 's a scientific law to be discerned here that producers would be well to heed : Mediocre movies start to drag as soon as the action speeds up\n",
      "stale and uninspired .\n",
      "and uninspired .\n",
      "uninspired .\n",
      ", it wears out its welcome as tryingly as the title character .\n",
      "could force you to scratch a hole in your head .\n",
      "could force you to scratch a hole in your head\n",
      "you to scratch a hole in your head\n",
      "convoluted\n",
      "violence\n",
      "Has no reason to exist , other than to employ Hollywood kids and people who owe favors to their famous parents .\n",
      "Nothing more than an amiable but unfocused bagatelle that plays like a loosely-connected string of acting-workshop exercises .\n",
      "unfocused\n",
      "dull\n",
      "Starts out mediocre , spirals downward , and thuds to the bottom of the pool with an utterly incompetent conclusion .\n",
      "spirals downward , and thuds to the bottom of the pool with an utterly incompetent conclusion .\n",
      "of the pool with an utterly incompetent conclusion\n",
      "the pool with an utterly incompetent conclusion\n",
      "with an utterly incompetent conclusion\n",
      "an utterly incompetent conclusion\n",
      "utterly incompetent conclusion\n",
      "utterly incompetent\n",
      "Horrendously amateurish filmmaking that is plainly dull and visually ugly when it is n't incomprehensible .\n",
      "filmmaking that is plainly dull and visually ugly when it is n't incomprehensible .\n",
      "filmmaking that is plainly dull and visually ugly when it is n't incomprehensible\n",
      "is plainly dull and visually ugly when it is n't incomprehensible\n",
      "plainly dull and visually ugly\n",
      "An awkwardly contrived exercise in magic realism\n",
      "'s a great deal of corny dialogue and preposterous moments\n",
      "of corny dialogue and preposterous moments\n",
      "corny dialogue and preposterous moments\n",
      "preposterous\n",
      "The film rehashes several old themes and is capped with pointless extremes -- it 's insanely violent and very graphic .\n",
      "is capped with pointless extremes\n",
      "capped with pointless extremes\n",
      "with pointless extremes\n",
      "pointless\n",
      "be disappointed\n",
      "never plays as dramatic even when dramatic things happen to people .\n",
      "generates plot points with a degree of randomness usually achieved only by lottery drawing .\n",
      "can depress you about life itself\n",
      "depress you\n",
      "smug self-satisfaction\n",
      "pomposity and pretentiousness\n",
      "It is depressing , ruthlessly pained and depraved , the movie equivalent of staring into an open wound .\n",
      "depressing , ruthlessly pained and depraved\n",
      "depressing\n",
      "depraved\n",
      "the movie equivalent of staring into an open wound\n",
      "instead opts for a routine slasher film that was probably more fun to make than it is to sit through .\n",
      "a routine slasher film that was probably more fun to make than it is to sit through\n",
      "Would 've been nice if the screenwriters had trusted audiences to understand a complex story , and left off the film 's predictable denouement .\n",
      "A superfluous sequel ... plagued by that old familiar feeling of ` let 's get this thing over with ' : Everyone has shown up at the appointed time and place , but\n",
      "A superfluous sequel ... plagued by that old familiar feeling of ` let 's get this thing over with ' : Everyone has shown up at the appointed time and place ,\n",
      "A superfluous sequel ... plagued by that old familiar feeling of ` let 's get this thing over with ' : Everyone has shown up at the appointed time and place\n",
      "A superfluous sequel\n",
      "superfluous sequel\n",
      "by that old familiar feeling of ` let 's get this thing over with '\n",
      "of ` let 's get this thing over with\n",
      "a familiar anti-feminist equation -LRB- career - kids = misery -RRB- in tiresome romantic-comedy duds\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing for Content\n",
    "List_Content = Df['Phrase'].to_list()\n",
    "Final_Article = []\n",
    "Complete_Content = []\n",
    "for article in List_Content:\n",
    "    print(article)\n",
    "    Processed_Content = text_preprocessing(article) #Cleaned text of Content attribute after pre-processing\n",
    "    Final_Article.append(Processed_Content)\n",
    "Complete_Content.extend(Final_Article)\n",
    "Df['Processed_Content'] = Complete_Content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cleaned_Data = Df.to_csv('Cleaned_Data.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
